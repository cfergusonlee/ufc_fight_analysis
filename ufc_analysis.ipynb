{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6e5513b6-ee20-4f63-aa05-75bb5028652d",
    "_uuid": "ca856cb388c65fcd41ca38e28fe7d6c4ff60a1a1"
   },
   "source": [
    "# UFC Fight Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2627be18-352a-422b-b206-906e2ad58ce8",
    "_uuid": "4d550657d3a62617db3061ae1660035d36b17423"
   },
   "source": [
    "\n",
    "## Todo\n",
    "\n",
    "\n",
    "1. Change landed and attempted data into percentages.\n",
    "2. Rewrite models to account for the unbalanced data.\n",
    "  1. Most fights are won by decision, so that should be accounted for\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Background\n",
    "\n",
    "## Data\n",
    "\n",
    "This data was forked from the [UFC Predictor and Notes](https://www.kaggle.com/calmdownkarm/ufc-predictor-and-notes) kaggle kernel. They scraped the data using Beautiful Soup and a Javascript API that pulled fighter data from ufc.com. The scripts pulled data from JSON objects and wrote them to a CSV. All data is from 2014 onwards and consists of fighter statistics merged with fight outcomes. They were unable to get data with the same level of detail in prior years, so all fighter records were reset to zero at the beginning of 2014 and built from there. It was an interesting project and I wanted to see if I could push it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e9262f56-9ff2-44d3-a361-886b823a6403",
    "_execution_state": "idle",
    "_uuid": "c548d6eec8b6bf6f090e43f5139fd9047a5755b8"
   },
   "outputs": [],
   "source": [
    "# Imports and Helper Functions\n",
    "# data Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rng\n",
    "from pprint import pprint\n",
    "\n",
    "# Web Scraping\n",
    "import json\n",
    "import codecs\n",
    "import csv\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import cStringIO\n",
    "import pprint\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#SciKit Learn Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n",
    "\n",
    "from subprocess import check_output\n",
    "print check_output([\"ls\", \"data\"]).decode(\"utf8\")\n",
    "data = pd.read_csv(\"data/data.csv\")\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "# Noteboook Functionality\n",
    "from IPython.core.interactiveshell import InteractiveShell # All statements are printed to output\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "380506f5-fa92-4c75-bc6e-7e5cc5583a93",
    "_execution_state": "idle",
    "_uuid": "e3eb27f2230353f40d1a892f6b1995cbdc421b99"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "There are 895 variables, so it might be worth looking through them to get a better sense of the data. Right now, I'm only aware of a handful of variables and have no idea how many missing values are in the dataset. At least listing the variable names will allow me to categorize them and possibly engineer new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = data.columns.values.tolist()\n",
    "pprint(var_list[:100])\n",
    "pprint(var_list[100:200])\n",
    "pprint(var_list[200:300])\n",
    "pprint(var_list[300:400])\n",
    "pprint(var_list[400:500])\n",
    "pprint(var_list[500:600])\n",
    "pprint(var_list[600:700])\n",
    "pprint(var_list[700:800])\n",
    "pprint(var_list[800:895])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "26da2dbd-ee3d-415b-ada0-c720b9edb9bc",
    "_uuid": "7fd571569d28c952dd5e68cce1304c5ee1652ebc"
   },
   "source": [
    "## Description\n",
    "From this, we can see that we have a total of 879 Columns and one dependent variable. \n",
    "The columns themselves have 4 integer types (Streaks, Previous Wins etc), 5 object types (Names, Winner - basically strings and arrays) and 870 Float types. \n",
    "This however does not give us a complete picture of our data, so we're using a few other pandas functions to get a better glimpse. \n",
    "We also had to engineer a few features that weren't available in the JSONs as explained in the data explanation in the introduction of the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ba3bfa6d-5e56-4bcd-8169-4d08e477e0cc",
    "_execution_state": "idle",
    "_uuid": "e29a904583efa8edd40c4e59880663737708d6f5"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4e46f917-7980-4700-a2f6-a546a0180563",
    "_execution_state": "idle",
    "_uuid": "7192b9d7480193f492e5a5926c6829964df60de8"
   },
   "outputs": [],
   "source": [
    "data.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=[np.int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = loc_data['B_Location'].str.split().apply(len).value_counts()\n",
    "count.index = count.index.astype(str) + ' words:'\n",
    "count.sort_index(inplace=True)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['winby'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['winner'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8b474058-f555-4ded-abc7-6293cd3de5c4",
    "_uuid": "c8f2e32828edc864d985ff84bc5e71dfb502a71a"
   },
   "source": [
    "### Some Notes to observe\n",
    "1. Red Side seems to win slightly more than blue (867/1477 = 58.7%)\n",
    "2. Donald Cerrone fights on Red side more than any other fighter, with 11 fights\n",
    "3. Tim Means fights on Blue side more than any other fighter with 8\n",
    "4. There are more fighters fighting debut fights. This statistic however could be skewed by the fact that our data set assumes debuts of every fighter in 2013\n",
    "5. Most Fights are won by decision, and 2015 had the most fights. \n",
    "6. The Most common hometown and training location for fighters is Rei De Janeiro in Brazil\n",
    "We also notice that 3 fighters don't have an age and 1 doesn't have a height. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fdd943bf-0a43-48e8-97ca-39e8d0d1244c",
    "_execution_state": "idle",
    "_uuid": "23ac3e73a2c898d4a95c8545bc6bb5e1900e0af5"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "54be1239-9bb7-4c10-b70c-3c36f0eed4bd",
    "_execution_state": "idle",
    "_uuid": "2c95c96b0230a9fb5b5a01fd9ada7da5f01e288a"
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e4fc5eb1-bce1-4cc2-8ce2-4c672e23abfb",
    "_uuid": "60b14f9a264e3e970ebf1c578354677049a4d6e2"
   },
   "source": [
    "## Data Cleaning\n",
    "\n",
    "The first step in data cleaning is to remove obvious outliers and columns that will not contribute to the model. One starting point is narrowing down the fights to just wins and losses, excluding no contests and draws. No contests are nearly impossible to predict as are draws, so it doesn't make sense to account for them. Here's a list of ideas so far:\n",
    "\n",
    "1. Draws or no contest ['winner']\n",
    "2. Blue and red ID\n",
    "3. Blue and red Name\n",
    "4. Blue and red Name\n",
    "5. Date\n",
    "\n",
    "The data is currently not in \"tidy\" format, so I may consider reshaping it. One obvious hint that this is the case is that every row has a round_4 and round_5 column, even though not all fights go to the last round. I'll see what I can do in terms of reshaping the data and look into whether it makes sense to do so. I wonder if there's a dplyr and tidyr package for Python? The R equivalents are pretty robust, so I'll probably start there. Here's an ongoing list of ideas that may be worth pursuing:\n",
    "\n",
    "1. Rearrange data to get rid of empty Round4 and Round5 data\n",
    "2. Separate city and country (Brazil vs RioDijanero Brazil and USA vs Stockton, California USA may add more predictive power)\n",
    "3. Change landed and attempted data into percentages. This may help make better comparisons across fighters making debuts vs fighters with established records.\n",
    "4. Add a column for submission wins, KO wins, etc\n",
    "\n",
    "I'm debating whether to turn the database into a database of fighters instead of a database of fights. One reason is that you have the accumulated statistics on each fighter's history going into each event. If you have Jon Jones matching up against DC, it may be useful to ask, \"What is it about Jon Jones' record that makes him a favorite? This may require a lot or rearranging, so it may be worth it to think this through a bit first.\n",
    "\n",
    "If I were to go this route, I would reset each fighter's stats and only add stats that came prior to each fight. For example, if Frankie Edgar fought in 2013 against Anderson Silva and then fought again against Matt Hughes in 2014, I'd want their fighter record inputs to be different. I think this may be a more comprehensive way of looking at the data and would align more closely to real-world applications. Here's a rough sketch of what the process might look like:\n",
    "\n",
    "1. Sort the dataframe by date\n",
    "2. Add each fighter's stats cumulatively based on prior fights\n",
    "  1. Figure out a way to accomplish this even though red is on one side and blue is on the other\n",
    "  2. Maybe put fighters on just red or blue? Might be impossible..\n",
    "  3. Maybe just account for this in the code? If B_Name == 'X' do this or if R_Name == 'X' do that\n",
    "3. Add columns for kicks taken, punches taken, etc?\n",
    "  1. One disadvantage is that this would mean nearly doubling the number of variables in a dataset that is already massive. This will hurt some machine learning models. Actually, this could lead to more variables than observations which would be no bueno, although it would still be possible to pare it down afterwards. Let's omit this for now and come back to it later if necessary.\n",
    "4. Consider consolidating round data. Why do you need 5 rounds of data for each fighter? Consider building a granular model and an aggragated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4012f27d-ea2e-4ea0-9a35-a58797ec9921",
    "_uuid": "531cd3e1d9d08349c92332571212ca0a3d5d6789"
   },
   "source": [
    "The head() and tail() functions give us a snapshot of the dataset's values. Not all rows and columns are represented, but it gives just enough to get a sense of how the data is organized. There are many NaN values and they need to be changed before passing the data onto the classifiers, but I want to be careful before making any changes. For instance, the winby column is extremely unbalanced and it may make sense to get rid of some of the outliers. For instance, there are a total of 16 draws and no-contests out of 1477. It makes no sense to include these since they are anomolies, so I will drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Country from B_Location and R_Location\n",
    "\n",
    "I want to extract each figher's country location. Currently there are 438 unique locations in the dataset. This is too many categories to get a sense of where fighters are training. There may also be some correlation between a fighter's location and his or her record. The first thing I did was exclude any fighters with null values for their current location. Thankfully, there were only 8 such cases. Then I cleaned up some of the mispellings / duplicate entries (USA and United States were consolidated into just USA). Finally, I replaced all [City Country] values to [Country] values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['B_Location'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['winby'].notnull()]\n",
    "loc_data = data[(data['B_Location'].notnull()) & data['R_Location'].notnull()]\n",
    "data = data[data['winner'] != 'no contest']\n",
    "#data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loc_data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = ['B_Location', 'R_Location']\n",
    "countries = ['Japan','Singapore']\n",
    "for location in locations:\n",
    "    for country in countries:\n",
    "        loc_data.loc[(loc_data[location] == country), location] = 'Unknown '+country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = ['B_Location', 'R_Location']\n",
    "c_dict = {\n",
    "    'United': 'USA',\n",
    "    'Brasil': 'Brazil',\n",
    "    'Englad': 'England',\n",
    "    'Czech': 'CzechRepublic',\n",
    "    'Moldova': 'Moldova',\n",
    "}\n",
    "\n",
    "post_c_dict = {\n",
    "    'Africa': 'South Africa',\n",
    "    'CzechRepublic': 'Czech Republic',\n",
    "    'PAN': 'Panama',\n",
    "    'Zealand': 'New Zealand'\n",
    "}\n",
    "countries = ['']\n",
    "for location in locations:\n",
    "    for k,v in c_dict.items():\n",
    "        loc_data.loc[loc_data[location].str.contains(k), location] = v\n",
    "new_cols = ['R_Country_Location', 'B_Country_Location']\n",
    "for new_col, location in zip(new_cols, locations):\n",
    "    loc_data[new_col] = loc_data[location].str.split().str[-1]\n",
    "for col in new_cols:\n",
    "    for k,v in post_c_dict.items():\n",
    "        loc_data.loc[loc_data[col].str.contains(k), col] = v\n",
    "\n",
    "for location, col in zip(locations, new_cols):\n",
    "    loc_data[location] = loc_data[col]\n",
    "    del loc_data[col]\n",
    "sorted(set(loc_data['B_Location'].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loc_data['B_Country_Location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f0f6efc0-d618-4fda-bbc4-437e25d4c23b",
    "_execution_state": "idle",
    "_uuid": "8ae33279e97416493a7c53b975077b5644e70929",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.fillna(value=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b01618ff-511b-4994-94d4-6a49143bd77f",
    "_execution_state": "idle",
    "_uuid": "66dbc6948c0d4095d81b2b1dcae338fc52c0ccc2"
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dcbf2cfa-5536-4a0e-b476-8af4395f6ed4",
    "_execution_state": "idle",
    "_uuid": "e1c922d8e97a8c018a394b6277421b7e632eef37"
   },
   "outputs": [],
   "source": [
    "dropdata = data.drop(['B_ID','B_Name','R_ID','R_Name','Date'],axis=1)\n",
    "dropdata.rename(columns={'BPrev':'B__Prev',\n",
    "                         'RPrev':'R__Prev',\n",
    "                         'B_Age':'B__Age',\n",
    "                         'B_Height':'B__Height',\n",
    "                         'B_Weight':'B__Weight',\n",
    "                         'R_Age':'R__Age',\n",
    "                         'R_Height':'R__Height',\n",
    "                         'R_Weight':'R__Weight',\n",
    "                         'BStreak':'B__Streak',\n",
    "                         'RStreak': 'R__Streak'},inplace=True)\n",
    "dropdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "16510a7a-a138-450f-a0f7-5790b934a6fa",
    "_execution_state": "idle",
    "_uuid": "293517e76b4cc146f6113100c2add1abd58cecb6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropdata.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdata.describe(include=[np.int])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "db811664-e0b5-4ae8-be12-385f0867051c",
    "_uuid": "df010f2f1a68472d4b6a6842669e9cd1bf5fa4a7"
   },
   "source": [
    "Next we need to convert our object types of columns into categorical columns. This is done to sort by logical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "09ee46c9-bd6f-4b7b-bc93-bfaf65e5b8ed",
    "_execution_state": "idle",
    "_uuid": "812983e28339ea7484b753e9e0bed7abed1a882e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objecttypes = list(dropdata.select_dtypes(include=['O']).columns)\n",
    "for col in objecttypes:\n",
    "    dropdata[col] = dropdata[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "121ee705-4017-494a-a4e0-d4c95a44e670",
    "_execution_state": "idle",
    "_uuid": "a66b1f5ee32454a9df0eeebae6fa6e9e5450856e"
   },
   "outputs": [],
   "source": [
    "cat_columns = dropdata.select_dtypes(['category']).columns\n",
    "dropdata[cat_columns] = dropdata[cat_columns].apply(lambda x: x.cat.codes)\n",
    "dropdata.info()\n",
    "dropdata.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "93cbf393-b9a9-43ef-aa3a-db8a1d622a42",
    "_uuid": "c14b77c6610f405be31c9bb108c250d0f1c82512"
   },
   "source": [
    "## Data Correlation\n",
    "While it would otherwise be normal practise to draw a heatmap or correlation matrix of our data to look for linear relationships, this is highly illogical due to the sheer number of features we're currently looking at.  Instead we examine the n largest correlations with our dependent variable (winner) to look for linear relationships - as you can see from the plot below, the relationships (if they exist) are highly non-linear. This suggests that alterations to our Data Set are required. \n",
    "\n",
    "Further, it seems that Round 4 statistics for the Red Fighter are the more correlating, this suggests that a split or delta sort of data set should produce better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19a7fdf5-c0ad-417f-a148-5d43fc898200",
    "_execution_state": "idle",
    "_uuid": "aa26e59b89257c4c59483118f4d419a431fddb94",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic Correlation Matrix\n",
    "# corrmat = data.corr()\n",
    "# f, ax = plt.subplots(figsize=(12, 9))\n",
    "# sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7559d19f-3556-45c3-967e-993e806a8a31",
    "_execution_state": "idle",
    "_uuid": "550d7a49e20c535fb76ed1909c78a685b7aef2c6"
   },
   "outputs": [],
   "source": [
    "# Subset Correlation Matrix\n",
    "k = 10 #number of variables for heatmap\n",
    "corrmat = dropdata.corr()\n",
    "cols = corrmat.nlargest(k, 'winner')['winner'].index\n",
    "cm = np.corrcoef(dropdata[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3c79b362-3492-4b63-a7ca-a1f9b574818b",
    "_uuid": "5779211f437c1c8b5959675de8d760934fc3074d"
   },
   "source": [
    "## Modeling\n",
    "We're evaluating the following models\n",
    "\n",
    "1. Perceptron\n",
    "2. Random Forests\n",
    "3. Decision Trees Classifier\n",
    "4. SGD Classifier\n",
    "5. Linear SVC\n",
    "6. Gaussian NB\n",
    "7. KNN\n",
    "\n",
    "I each model's random_state when appropriate and set the SVM's class_weight to balanced to account for the unbalanced data. There's a lot that can be done in terms of tuning the hyperparameters. I may have to come back to this later to further test different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(sklearn.ensemble.RandomForestClassifier())\n",
    "# help(sklearn.naive_bayes.GaussianNB)\n",
    "help(sklearn.svm.LinearSVC)\n",
    "# import sklearn\n",
    "# help(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2358b514-cb05-4acc-972d-c6668c3faa08",
    "_execution_state": "idle",
    "_uuid": "b848bf019778bab073e13f644bfe0f2d35a0d4e0"
   },
   "outputs": [],
   "source": [
    "# We Store prediction of each model in our dict\n",
    "# Helper Functions for our models. \n",
    "\n",
    "def percep(X_train,Y_train,X_test,Y_test,Models):\n",
    "    perceptron = Perceptron(max_iter = 1000, tol = 0.001, random_state=42)\n",
    "    perceptron.fit(X_train, Y_train)\n",
    "    Y_pred = perceptron.predict(X_test)\n",
    "    Models['Perceptron'] = [accuracy_score(Y_test,Y_pred),confusion_matrix(Y_test,Y_pred)]\n",
    "    return\n",
    "\n",
    "def ranfor(X_train,Y_train,X_test,Y_test,Models):\n",
    "    randomfor = RandomForestClassifier(max_features=\"sqrt\",\n",
    "                                       n_estimators = 700,\n",
    "                                       max_depth = None,\n",
    "                                       n_jobs=-1,\n",
    "                                       random_state=42\n",
    "                                      )\n",
    "    randomfor.fit(X_train,Y_train)\n",
    "    Y_pred = randomfor.predict(X_test)\n",
    "    Models['Random Forests'] = [accuracy_score(Y_test,Y_pred),confusion_matrix(Y_test,Y_pred)]\n",
    "    return\n",
    "\n",
    "def dec_tree(X_train,Y_train,X_test,Y_test,Models):\n",
    "    decision_tree = DecisionTreeClassifier(class_weight=\"balanced\",random_state=42)\n",
    "    decision_tree.fit(X_train, Y_train)\n",
    "    Y_pred = decision_tree.predict(X_test)\n",
    "    Models['Decision Tree'] = [accuracy_score(Y_test,Y_pred),confusion_matrix(Y_test,Y_pred)]\n",
    "    return\n",
    "\n",
    "def SGDClass(X_train,Y_train,X_test,Y_test,Models):\n",
    "    sgd = SGDClassifier(max_iter = 1000, tol = 0.001, class_weight = \"balanced\", random_state=42)\n",
    "    sgd.fit(X_train, Y_train)\n",
    "    Y_pred = sgd.predict(X_test)\n",
    "    Models['SGD Classifier'] = [accuracy_score(Y_test,Y_pred),confusion_matrix(Y_test,Y_pred)]\n",
    "    return\n",
    "\n",
    "def linSVC(X_train,Y_train,X_test,Y_test,Models):\n",
    "    linear_svc = LinearSVC(class_weight=\"balanced\", random_state=42)\n",
    "    linear_svc.fit(X_train, Y_train)\n",
    "    Y_pred = linear_svc.predict(X_test)\n",
    "    Models['SVM'] = [accuracy_score(Y_test,Y_pred),confusion_matrix(Y_test,Y_pred)]\n",
    "    return\n",
    "\n",
    "def bayes(X_train,Y_train,X_test,Y_test,Models):\n",
    "    gaussian = GaussianNB()\n",
    "    gaussian.fit(X_train, Y_train)\n",
    "    Y_pred = gaussian.predict(X_test)\n",
    "    Models['Bayes'] = [accuracy_score(Y_test,Y_pred),confusion_matrix(Y_test,Y_pred)]\n",
    "    return\n",
    "\n",
    "def Nearest(X_train,Y_train,X_test,Y_test,Models):\n",
    "    knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    Y_pred = knn.predict(X_test)\n",
    "    Models['KNN'] = [accuracy_score(Y_test,Y_pred),confusion_matrix(Y_test,Y_pred)]\n",
    "\n",
    "def run_all_and_Plot(df):\n",
    "    Models = dict()\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_all = df.drop(['winner'], axis=1)\n",
    "    y_all = df['winner']\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=0)\n",
    "    percep(X_train,Y_train,X_test,Y_test,Models)\n",
    "    ranfor(X_train,Y_train,X_test,Y_test,Models)\n",
    "    dec_tree(X_train,Y_train,X_test,Y_test,Models)\n",
    "    SGDClass(X_train,Y_train,X_test,Y_test,Models)\n",
    "    linSVC(X_train,Y_train,X_test,Y_test,Models)\n",
    "    bayes(X_train,Y_train,X_test,Y_test,Models)\n",
    "    Nearest(X_train,Y_train,X_test,Y_test,Models)\n",
    "    return Models\n",
    "\n",
    "\n",
    "def plot_bar(dict):\n",
    "    labels = tuple(dict.keys())\n",
    "    y_pos = np.arange(len(labels))\n",
    "    values = [dict[n][0] for n in dict]\n",
    "    plt.bar(y_pos, values, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, labels,rotation='vertical')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy of different models')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_cm(dict):\n",
    "    count = 1\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    for model in dict:\n",
    "        cm = dict[model][1]\n",
    "        labels = ['W','L']\n",
    "        ax = fig.add_subplot(4,4,count)\n",
    "        cax = ax.matshow(cm)\n",
    "        plt.title(model,y=-0.8)\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticklabels([''] + labels)\n",
    "        ax.set_yticklabels([''] + labels)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        # plt.subplot(2,2,count)\n",
    "        count+=1\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "42484153-a9ac-4b08-8b20-33fef52b7109",
    "_execution_state": "idle",
    "_uuid": "3152fd66ab1197f486355c1c742767ab8960f2b2"
   },
   "outputs": [],
   "source": [
    "accuracies = run_all_and_Plot(dropdata)\n",
    "CompareAll = dict()\n",
    "CompareAll['Baseline'] = accuracies\n",
    "for key,val in accuracies.items():\n",
    "    print(str(key) +' '+ str(val[0]))\n",
    "plot_bar(accuracies)\n",
    "plot_cm(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a8c2e442-8ba8-485e-b0c7-db66e4a29909",
    "_uuid": "03dca6326695b2bfd58222eece36c75c315f39c9"
   },
   "source": [
    "Theoretically, we should get best results from our Random Forests Model, thus attempting to tune hyper parameters using GridSearch from Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "36458454-c9bd-4490-8c67-81395c60e44b",
    "_execution_state": "idle",
    "_uuid": "1ece9ef484f1dccdc754f106f094adb9f5d3b0ab"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "#X_all = dropdata.drop(['winner'], axis=1)\n",
    "#y_all = dropdata['winner']\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=23)\n",
    "#rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True, max_depth=None) \n",
    "#param_grid = { \n",
    "#    'n_estimators': [200,700],\n",
    "#    'max_features': ['auto', 'sqrt', 'log2']\n",
    "#}\n",
    "\n",
    "#CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "#CV_rfc.fit(X_train, Y_train)\n",
    "#print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cff0584f-23fa-47dc-af33-0d1e9fefee61",
    "_uuid": "77432b8dbe0ab61054cc75eff306f5fa468c54ad"
   },
   "source": [
    "### Trying to improve results by dividing features\n",
    "\n",
    "This block turns each individual red and blue round stat into a ratio of red to blue values. It divides the number of features from 895 to 450. For instance, it turns R_Round4_Strikes_Kicks_Landed and B_Round4_Strikes_Kicks_Landed into a single ratio of red to blue strikes kicks landed. Interesting approach, it appears to be part of th iteration process to see what will be most effective in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19fed020-cd6f-4e96-88f1-3bb5fcc6d89c",
    "_execution_state": "idle",
    "_uuid": "929313bed2a81c876094b2a9561dbe2a29bde64c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dontchange = ['winner','Event_ID','Fight_ID','Max_round','Last_round','B_Age','R_Age']\n",
    "numeric_cols = [col for col in dropdata if col not in dontchange]\n",
    "dropdata[numeric_cols] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ceb06d96-db42-4eb6-a69b-9f768915327d",
    "_execution_state": "idle",
    "_uuid": "984bd6d4d6befa7292782a2dc2901cde876d09fc"
   },
   "outputs": [],
   "source": [
    "newDF = dropdata.copy()\n",
    "blue_cols = [col for col in dropdata.columns if 'B__' in col]\n",
    "red_cols = [col for col in dropdata.columns if 'R__' in col]\n",
    "for (blue,red) in zip(blue_cols,red_cols):\n",
    "    newkey = ''.join(str(blue).split('_')[2:])\n",
    "    dropdata[newkey] = dropdata[str(blue)]/dropdata[str(red)]\n",
    "    del dropdata[str(blue)]\n",
    "    del dropdata[str(red)]\n",
    "newDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8a3a3718-685b-4e63-a766-ab7cc1bdc908",
    "_execution_state": "idle",
    "_uuid": "b21918fc4f94af7cbe2835af0652bb4968afb03f"
   },
   "outputs": [],
   "source": [
    "accuracies = run_all_and_Plot(dropdata)\n",
    "for key,val in accuracies.items():\n",
    "    print(str(key) +' '+ str(val[0]))\n",
    "CompareAll['Blue/Red'] = accuracies\n",
    "plot_bar(accuracies)\n",
    "plot_cm(accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a7f9eae-66be-4c19-b127-922f10767673",
    "_uuid": "ea63206fcf4568e59a108845e911c3f74dd2bd11"
   },
   "source": [
    "#### Dropping Round 4 and Round 5 since most fights are 3 round Max.\n",
    "\n",
    "This block drops round 4 and round 5 columns from the dataset but keeps fights that last 5 rounds. It may be an attempt to overcome the amount of null values in the dataset. I'd go a different route since this doesn't seem very precise. I'd tidy and consolidate the data from all rounds into one so there wouldn't be a need to drop round 4 and 5 data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "787f1bcd-5abb-4001-8e2c-efd68250274b",
    "_execution_state": "idle",
    "_uuid": "134855cbdb329cbc78ce9c0943fd92591316b963"
   },
   "outputs": [],
   "source": [
    "r4 = [col for col in dropdata.columns if \"Round4\" in col]\n",
    "r5 = [col for col in dropdata.columns if \"Round5\" in col]\n",
    "threerounds = dropdata.drop(r4+r5,axis = 1)\n",
    "accuracies = run_all_and_Plot(threerounds)\n",
    "for key,val in accuracies.items():\n",
    "    print(str(key)+' '+str(val[0]))\n",
    "CompareAll['DropR4&R5'] = accuracies\n",
    "plot_bar(accuracies)\n",
    "plot_cm(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "95705518-197c-4460-93e2-afb5ffcf9b2d",
    "_uuid": "d66cf5933dd7ecb5c5c8864e3af0076361d14f39"
   },
   "source": [
    "#### Dropping 5 round fights entirely\n",
    "\n",
    "This block drops all title fights. I think this is a mistake for a couple reasons. The dataset is small enough already (only 1477 observations) so dropping any records would probably have a negative impact on the machine learning algorithm. Also, there may be a better way to represent the data that would negate the reasoning behind this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "33904436-d028-48bb-b5df-04de18a1d757",
    "_execution_state": "idle",
    "_uuid": "ad78f1b01511636b4304ac354756cdcc786cd032"
   },
   "outputs": [],
   "source": [
    "foobar = threerounds.loc[threerounds['Max_round'] == 3]\n",
    "bewb = threerounds.drop(['Max_round','Last_round'],axis=1)\n",
    "accuracies = run_all_and_Plot(bewb)\n",
    "for key,val in accuracies.items():\n",
    "    print(str(key)+' '+str(val[0]))\n",
    "CompareAll['Drop5RoundFights'] = accuracies\n",
    "plot_bar(accuracies)\n",
    "plot_cm(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92e4152a-b7d1-49b8-b4aa-2bf527db625c",
    "_uuid": "15d6056717d18863b0d6d5e37f11676e9d50a379"
   },
   "source": [
    "### Dropping First Fights\n",
    "\n",
    "This drops any fights in which the fighter has no previous recorded. I may use this in my model but again I'm hesitant to do so because it involves a huge loss of data. Looking at the original CSV, there are 342 records in for Red and 499 records for blue in which they have no previously recorded fight data. I can't afford to lose more than half the data.\n",
    "\n",
    "This is actually a pretty interesting problem because I'm not sure how to deal with new fighters. By definition, their stats will show up as zeros if they don't have any fight data. It might be worth exploring how to get records prior to 2014 and going from there. Possible sources include:\n",
    "\n",
    "1. Fightmetrics\n",
    "2. Fight data from other fight organizations\n",
    "3. Looking into what the original analyst meant when he said the previous data was not as granular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.RPrev == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.BPrev == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b7541f52-8875-4bf1-a0d3-18cf8cae67b9",
    "_execution_state": "idle",
    "_uuid": "f50f278e398522fe6f0c9df8f149ee43695b7b01"
   },
   "outputs": [],
   "source": [
    "blahblah = bewb[bewb.Prev != 1]\n",
    "accuracies = run_all_and_Plot(blahblah)\n",
    "for key,val in accuracies.items():\n",
    "    print(str(key)+' '+str(val[0]))\n",
    "CompareAll['DroppingDebut'] = accuracies\n",
    "plot_bar(accuracies)\n",
    "plot_cm(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Round Stats\n",
    "\n",
    "This block sums the stats for each round into one value. For example, it combines round 1-5 strikes_landed into a single value. This is what I was thinking of doing. May have to borrow this code in my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c8eab9e2-9b22-422c-9856-a08652d39ac1",
    "_execution_state": "idle",
    "_uuid": "b1d4556071d9f065d72433720955e33ba2c91497"
   },
   "outputs": [],
   "source": [
    "blue_cols\n",
    "newDF.info()\n",
    "b_feats = list(set([x[10:] for x in blue_cols if \"Round\" in x]))\n",
    "r_feats = list(set([x[10:] for x in red_cols if \"Round\" in x]))\n",
    "def sumshit(b_feats,cols):\n",
    "    for x in b_feats:\n",
    "        newDF.loc[:,x] = 0\n",
    "        for y in cols:\n",
    "            if x in y:\n",
    "                newDF[x] += newDF[y]\n",
    "                newDF.drop(y,axis=1,inplace=True)\n",
    "sumshit(b_feats,blue_cols)\n",
    "sumshit(r_feats,red_cols)\n",
    "newDF.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3926c4b2-8f38-443d-9e0f-a0c7c35c7b05",
    "_execution_state": "idle",
    "_uuid": "f7b198db886b7fccd5e6573aea33303f1b53e931"
   },
   "outputs": [],
   "source": [
    "newDF.describe()\n",
    "accuracies = run_all_and_Plot(newDF)\n",
    "for key,val in accuracies.items():\n",
    "    print(str(key) +' '+ str(val[0]))\n",
    "CompareAll['SumRounds'] = accuracies\n",
    "plot_bar(accuracies)\n",
    "plot_cm(accuracies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Red to Blue\n",
    "\n",
    "This block compares red stats to blue stats. It creates a ratio of strikes landed by red vs strikes landed by blue for each category. Very useful. I think I'm going to borrow this code as well since it's an idea I was thinking about implementing in my reanalysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f1338ea2-1811-42a7-856b-fa74f655b5a7",
    "_execution_state": "idle",
    "_uuid": "0133b07cdef4add8e5cb1a52707fdcf2b29d234d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blue_cols = [col for col in newDF.columns if 'B__' in col]\n",
    "red_cols = [col for col in newDF.columns if 'R__' in col]\n",
    "for (blue,red) in zip(blue_cols,red_cols):\n",
    "    newkey = ''.join(str(blue).split('_')[2:])\n",
    "    newDF[newkey] = newDF[str(blue)]/newDF[str(red)]\n",
    "    del newDF[str(blue)]\n",
    "    del newDF[str(red)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1ca5d30c-54c9-4ab3-82a6-cdfed729e479",
    "_execution_state": "idle",
    "_uuid": "e2c770779f3ad517aaa985c706551fceec266221"
   },
   "outputs": [],
   "source": [
    "accuracies = run_all_and_Plot(newDF)\n",
    "for key,val in accuracies.items():\n",
    "    print(str(key) +' '+ str(val[0]))\n",
    "CompareAll['SumRounds'] = accuracies\n",
    "plot_bar(accuracies)\n",
    "plot_cm(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Features\n",
    "\n",
    "This block drops features that are seemingly arbitrary and may have little effect on the data. It drops weight, hometown, event location, event id, fight id, max round and last round. Interestingly, the accuracy scores dip slightly after this is done. I'm not sure I agree with the decisions made here. For instance, I still want to take a look at splitting the columns by city and country. I'm not sure about the others. I'll have to combe back to this later down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4e399915-898f-4ab7-960b-690042a93f6c",
    "_execution_state": "idle",
    "_uuid": "94a56148aca4bc655515e369b11e3c5b5671925f"
   },
   "outputs": [],
   "source": [
    "reduced_features = newDF.drop([\"Weight\",\"B_HomeTown\",\"B_Location\", \"Event_ID\", \"Fight_ID\", \"Max_round\", \"Last_round\", \"R_HomeTown\", \"R_Location\"],axis = 1)\n",
    "accuracies = run_all_and_Plot(reduced_features)\n",
    "for key,val in accuracies.items():\n",
    "    print(str(key) +' '+ str(val[0]))\n",
    "CompareAll['Reduced Features'] = accuracies\n",
    "plot_bar(accuracies)\n",
    "plot_cm(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "751ceb20a9476237c792a87034de91e5bcbbcb71"
   },
   "outputs": [],
   "source": [
    "reduced_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(reduced_features.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "adce3876-6f57-4501-8650-91ccc6d7d7ab",
    "_uuid": "1fdbd098b562a7eee62122583bc083f9610439a0"
   },
   "source": [
    "## Conclusion\n",
    "Our model has a best predictive value that lies between 58-63% on average between runs. Despite a very low accuracy model, we believe this is the best possible given the amount of available data and its inherent noise. \n",
    "\n",
    "## Stretch Goals\n",
    "\n",
    "### Rewrite and re-run scraper to pull data from earlier years\n",
    "\n",
    "It won't be as granular, but it may strengthen the analysis. There might not be enough records from 2014 onwards to model the data accurately. On the other hand, I'm not even sure the previous analyst modeled the data accurately. He took fight data from each fight and used that to predict the winner. That's not realistic. You don't have a round-by-round analysis before the fight happens! It would be much better to use the running totals to predict a fight's outcome. Looks like we're learning Beautiful Soup! At least you have a starting point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scraper Rewrite\n",
    "\n",
    "The following scraper retrieves data from ufc.com. It currently pulls a single fighter's data. I added functionality to convert that data into a JSON file, extract the list of fights, and write that to a CSV but it's still incomplete. Still need the following steps:\n",
    "\n",
    "1. Pull from list of all fighters\n",
    "2. Merge all fight data so opponent data shows up\n",
    "3. Flatten data where appropriate\n",
    "4. Format data appropriately\n",
    "\n",
    "I'm scrapping this and building my own scraper. The data on fightmetric is more robust. The data I'm getting from ufc.com has a lot of missing data in general and it does not break down each fight as well. It lists strike percentages but does not give raw numbers. The fightmetric data lists raw data as well as percentages and may lead to a more complete picture of each fighter. It also does not require as much merging. Looks like we're starting fresh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 http://www.fightmetric.com/fight-details/d1bee15e60ee6721\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import os\n",
    "os.chdir('/Users/courtneyfergusonlee/ufc_fight_analysis/MMA-scraper-master')\n",
    "\n",
    "# Load URLs from CSV (created in fightmetric_scraper.py)\n",
    "fight_urls = pd.read_csv('fight urls.csv', encoding='utf-8')['link'].values.tolist()\n",
    "\n",
    "# Initialize an empty dataframe\n",
    "fighter_df = pd.DataFrame(columns=['name_first', 'name_last', 'kd', 'sig_strikes', 'sig_attempts', 'strikes', 'strike_attempts', \n",
    "                                   'takedowns', 'td_attempts', 'sub_attempts', 'pass', 'reversals', 'head', 'head_attempts', 'body', \n",
    "                                   'body_attempts','leg', 'leg_attempts', 'distance', 'distance_attempts', 'clinch', 'fight_id',\n",
    "                                   'clinch_attempts', 'ground', 'ground_attempts', 'win/loss', 'referee', 'round', 'method'])\n",
    "\n",
    "\n",
    "# Iterate through the fight urls, and pull relevant variables/fields\n",
    "for i in range(len(fight_urls)):\n",
    "    if i%50==0:\n",
    "        print i, fight_urls[i]\n",
    "    \n",
    "    sock = urllib.urlopen(fight_urls[i]) # specific URL for a fight\n",
    "    fight_html = sock.read()\n",
    "    fight_soup = BeautifulSoup(fight_html, \"html.parser\")\n",
    "    trs = fight_soup.find_all('tr') # all the tables in each fight URL\n",
    "    headers = fight_soup.find_all('i')\n",
    "    bad_call = 0\n",
    "    try: \n",
    "        referee = str(headers[24].get_text()).split()[1] + ' ' + str(headers[24].get_text()).split()[-1]\n",
    "    except:\n",
    "        referee = None\n",
    "    try:\n",
    "        rounds = str(headers[18].get_text()).split()[1]\n",
    "    except:\n",
    "        rounds = None\n",
    "    try:\n",
    "        method = str(headers[17].get_text()).split()[0]\n",
    "    except:\n",
    "        method = None\n",
    "    try:\n",
    "        tr1 = str(trs[1].get_text()).split()\n",
    "        # Find the location of the 2nd table tr2 (it varies)\n",
    "        j = 0\n",
    "        while j < 10:\n",
    "            if str(trs[j].get_text()).split()[6] == 'Head':\n",
    "                #print j+1\n",
    "                tr2 = str(trs[j+1].get_text()).split()\n",
    "                j = 10\n",
    "            else:\n",
    "                j += 1\n",
    "        #print tr1; #print tr2\n",
    "        \n",
    "        # Test for the end of names\n",
    "        k = 0\n",
    "        while k < len(tr1):\n",
    "            try:\n",
    "                int(tr1[k])\n",
    "                break\n",
    "            except:\n",
    "                k += 1\n",
    "                continue\n",
    "        #print k\n",
    "    except:\n",
    "        print str(i) + ' bad call' if i%20 == 0 else None\n",
    "        bad_call += 1\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Add each fighter's information to the dataframe\n",
    "    fighter1 = pd.DataFrame({'name_first': tr1[:1], 'name_last': tr1[1:2], 'kd': tr1[k], 'sig_strikes': tr1[k+2],\n",
    "    'sig_attempts': tr1[k+4], 'strikes': tr1[k+10], 'strike_attempts': tr1[k+12], 'takedowns': tr1[k+16],'td_attempts': tr1[k+18],\n",
    "    'sub_attempts': tr1[k+24], 'pass': tr1[k+26], 'reversals': tr1[k+28], 'head': tr2[k+8], 'head_attempts': tr2[k+10],\n",
    "    'body': tr2[k+14], 'body_attempts': tr2[k+16], 'leg': tr2[k+20], 'leg_attempts': tr2[k+22], 'distance': tr2[k+26],\n",
    "    'distance_attempts': tr2[k+28], 'clinch': tr2[k+32], 'clinch_attempts': tr2[k+34], 'ground': tr2[k+38], \n",
    "    'ground_attempts': tr2[k+40], 'win/loss': 1, 'referee': referee, 'round': rounds, 'method': method, 'fight_id': i})\n",
    "\n",
    "    fighter2 = pd.DataFrame({'name_first': tr1[2:3], 'name_last': tr1[3:4], 'kd': tr1[k+1], 'sig_strikes': tr1[k+5], \n",
    "    'sig_attempts': tr1[k+7], 'strikes': tr1[k+13], 'strike_attempts': tr1[k+15], 'takedowns': tr1[k+19],'td_attempts': tr1[k+21],\n",
    "    'sub_attempts': tr1[k+25], 'pass': tr1[k+27], 'reversals': tr1[k+29], 'head': tr2[k+11], 'head_attempts': tr2[k+13],\n",
    "    'body': tr2[k+17], 'body_attempts': tr2[k+19], 'leg': tr2[k+23], 'leg_attempts': tr2[k+25], 'distance': tr2[k+29],\n",
    "    'distance_attempts': tr2[k+31], 'clinch': tr2[k+35], 'clinch_attempts': tr2[k+37], 'ground': tr2[k+41], \n",
    "    'ground_attempts': tr2[k+43], 'win/loss': 0, 'referee': referee, 'round': rounds, 'method': method, 'fight_id': i})\n",
    "    \n",
    "    fighter_df = pd.concat([fighter_df, fighter1, fighter2], axis=0, ignore_index=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fighter_df.to_csv('fights.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fight_urls = pd.read_csv('fight urls.csv', encoding='utf-8')\n",
    "fight_urls = fight_urls['link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fight_urls.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fighter_df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
